{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Goals\n",
    "\n",
    "In Assignment 1, we studied how information is represented by a single spiking neuron. In this assignment, you will learn how to construct networks of spiking neurons for a given cognitive task, how to propagate information through a network, and understanding the intuition behind network design choices.Â \n",
    "\n",
    "Let's first import all the libraries required for this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: From single neuron to network of neurons\n",
    "## 1a.\n",
    "What computational advantages do networks of neurons offer when compared against information processing by a single neuron? In other words, why do we need networks of neurons? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1a.\n",
    "> Network of neurons helps us to capture more information about the input and also keep computational overhead in check which otherwise overshoots when we use single neuron for more timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. \n",
    "Describe the algorithm for the information flow through a network of spiking leaky-integrate-and-fire (LIF) neurons. Specifically, trace out the steps required to compute network output from a given (continuous-valued) inputs. The algorithm should describe how continuous-valued inputs are fed to the SNN input layer, how the layer activations are computed, and how the output layer activity is decoded. Also, provide a diagrammatic overview of the algorithm to aid your explanation. You are free to assume any network size, and input and output dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1b.\n",
    "1. Encoding Input\n",
    "    - Input Image is initially encoded using one of the encoding methods, like rate encoding or temporal encoding.\n",
    "    - Initially we generate a random image of same dimensions as input image for specified number of timesteps. This random image is compared with original input image pixels and converted to spikes [1] whenever image pixels values are greater tha random image pixel, and no spikes [0] otherwise.\n",
    "2. Feed Forward Propagation\n",
    "    - Using the input spikes we calculate the current for next layer: \n",
    "        - Compute the current for the hidden layers using layer weights: psp = weight * input\n",
    "    - Integrate the current (psp) into membrane voltage to calculate spikes for next layer: \n",
    "        - v[t] = dv.v[t-1] + psp[t]\n",
    "    - use thresholding to convert voltage to spikes: \n",
    "        - if v[t] >= vth: \n",
    "            - generate spikes for neuron\n",
    "            - Reset v to 0\n",
    "    - accumulate output spikes until specified number of timesteps\n",
    "3. Decode Output using arithmetic operations and argmax of python numpy library\n",
    "\n",
    "\n",
    "<img src=\"SNN.jpeg\" width=700 height=550 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Elements of Constructing Feedforward Networks\n",
    "In this exercise, you will implement the two fundamental components of a feedforward spiking neural network: i) layers of neurons and ii) connections between those layers\n",
    "## 2a. \n",
    "As the first step towards creating an SNN, we will create a class that defines a layer of LIF neurons. The layer object creates a collection of LIF neurons and applies input current to it (also called psp_input for postsynaptic input) to produce the collective spiking output of the layer. \n",
    "\n",
    "Below is the class definition for a layer of LIFNeurons. Fill in the components to define the layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFNeurons:\n",
    "    \"\"\" Define Leaky Integrate-and-Fire Neuron Layer \"\"\"\n",
    "\n",
    "    def __init__(self, dimension, vdecay, vth):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dimension (int): Number of LIF neurons in the layer\n",
    "            vdecay (float): voltage decay of LIF neurons\n",
    "            vth (float): voltage threshold of LIF neurons\n",
    "        \n",
    "        This function is complete. You do not need to do anything here.\n",
    "        \"\"\"\n",
    "        self.dimension = dimension\n",
    "        self.vdecay = vdecay\n",
    "        self.vth = vth\n",
    "\n",
    "        # Initialize LIF neuron states\n",
    "        self.volt = np.zeros(self.dimension)\n",
    "        self.spike = np.zeros(self.dimension)\n",
    "    \n",
    "    def __call__(self, psp_input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            psp_input (ndarray): synaptic input current at a single timestep. The shape of this is same as the number of neurons in the layer. \n",
    "        Return:\n",
    "            self.spike: output spikes from the layer. The shape of this should be the same as the number of neurons in the layer. \n",
    "        \n",
    "        Write the expressions for updating the voltage and generating the spikes for the layer given psp_input at one timestep. \n",
    "        \"\"\"\n",
    "        #Update the voltage\n",
    "        self.volt = (self.vdecay*self.volt) + psp_input\n",
    "        \n",
    "        #Generate the spikes from the voltage \n",
    "        self.spike = (self.volt >= self.vth).astype(int)\n",
    "        \n",
    "        #Reset the voltage if the neuron spikes\n",
    "        self.volt[self.volt >= self.vth] = 0\n",
    "        \n",
    "        return self.spike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the correctness of your class implementation, create a layer of neurons using the class definition above, and pass through it random inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input spike: [0 1 1 1 1 1 0 0 1 1]\n",
      "Output spike: [0 1 1 1 1 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "#Create a layer of neurons using the class definition above. You can pick any parameter values for the neurons.\n",
    "dimension = 10\n",
    "vdecay = 0.5\n",
    "vth = 0.5\n",
    "layer = LIFNeurons(dimension, vdecay, vth)\n",
    "\n",
    "#Create random input spikes with any probability and print them. Numpy random.choice function might be useful here. \n",
    "p = 0.6\n",
    "in_spikes = np.random.choice([0,1], dimension, True, [1-p, p])\n",
    "print(\"Input spike: {}\".format(in_spikes))\n",
    "\n",
    "#Propagate the random input spikes through the layer and print the output\n",
    "out_spikes = layer(in_spikes)\n",
    "print(\"Output spike: {}\".format(out_spikes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b.\n",
    "Now, we will create a class the defines the connection between a presynaptic layer and a postsynaptic layer. To create the connection, we need the activity of the presynaptic layer (also called presynaptic layer activation) and the weight matrix connecting the presynaptic and postsynaptic neurons. The output of the class should be the current for the postsynaptic layer. \n",
    "\n",
    "Below is the class definition for Connections. Fill in the components to create the connections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connections:\n",
    "    \"\"\" Define connections between spiking neuron layers \"\"\"\n",
    "\n",
    "    def __init__(self, weights, pre_dimension, post_dimension):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pre_dimension (int): number of neurons in the presynaptic layer\n",
    "            post_dimension (int): number of neurons in the postsynaptic layer\n",
    "            weights (ndarray): connection weights of shape post_dimension x pre_dimension\n",
    "\n",
    "        This function is complete. You do not need to do anything here.\n",
    "\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.pre_dimension = pre_dimension\n",
    "        self.post_dimension = post_dimension\n",
    "    \n",
    "    def __call__(self, spike_input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spike_input (ndarray): spikes generated by the pre-synaptic neurons\n",
    "        Return:\n",
    "            psp: current for the post-synaptic neurons\n",
    "        \n",
    "        Write the operation for computing psp\n",
    "        \"\"\"\n",
    "        \n",
    "        #Compute psp given spike_input and self.weights\n",
    "        \n",
    "        psp = spike_input @ np.transpose(self.weights)\n",
    "        \n",
    "        return psp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the correctness of your class implementation, create a connection object and compute the postsynaptic current for random presynaptic activation inputs and random connection weights. You can pick arbitrary values for class arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postsynaptic current shape: (8,)\n"
     ]
    }
   ],
   "source": [
    "#Define the dimensions of the presynaptic layer in a variable\n",
    "pre_dim = 25\n",
    "\n",
    "#Define the dimensions of the postsynaptic layer in a variable\n",
    "post_dim = 8\n",
    "\n",
    "#Create random presynaptic inputs with any probability. Numpy random choice function might be useful here. \n",
    "presynaptic_current = np.random.choice([0,1], pre_dim, True, [1-p, p])  #p = 0.6\n",
    "# print(\"presynaptic current: {}\".format(presynaptic_current))\n",
    "\n",
    "#Create a random connection weight matrix. Numpy random rand function might be useful here. \n",
    "w_0 = np.random.rand(post_dim, pre_dim)\n",
    "# print(\"weight at input layer, w_0: {}\".format(w_0))\n",
    "\n",
    "#Initialize a connection object using the Connection class definition and pass the variables created above as arguments\n",
    "connect = Connections(w_0, pre_dim, post_dim)\n",
    "\n",
    "#Compute the current for the postsynaptic layer when the connection object is fed random presynaptic activation inputs\n",
    "postsynaptic_current = connect(presynaptic_current)\n",
    "\n",
    "#Print the shape of the current\n",
    "print(\"Postsynaptic current shape: {}\".format(postsynaptic_current.shape))\n",
    "# print(\"postsynaptic current: {}\".format(postsynaptic_current))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Constructing Feedforward SNN\n",
    "Now that you have implemented the basic elements of an SNN- layer and connection, you are all set to implement a fully functioning SNN. The SNN that you will implement here consists of an input layer, a hidden layer, and an output layer. \n",
    "\n",
    "Below is the class definition of an SNN. Your task is to create the layers and connections that form the network using the class definitions in Question 2. Then complete the function to propagate a given input through the network and decode network output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN:\n",
    "    \"\"\" Define a Spiking Neural Network with One Hidden Layer \"\"\"\n",
    "\n",
    "    def __init__(self, input_2_hidden_weight, hidden_2_output_weight, \n",
    "                 input_dimension=784, hidden_dimension=256, output_dimension=10,\n",
    "                 vdecay=0.5, vth=0.5, snn_timestep=20):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_2_hidden_weight (ndarray): weights for connection between input and hidden layer. dimension should be hidden_dimension x input_dimension. \n",
    "            hidden_2_output_weight (ndarray): weights for connection between hidden and output layer. dimension should be output dimension x hidden dimension. \n",
    "            input_dimension (int): number of neurons in the input layer\n",
    "            hidden_dimension (int): number of neurons in the hidden layer\n",
    "            output_dimension (int): number of neurons in the output layer\n",
    "            vdecay (float): voltage decay of LIF neurons\n",
    "            vth (float): voltage threshold of LIF neurons\n",
    "            snn_timestep (int): number of timesteps for simulating the network (also called inference timesteps)\n",
    "        \"\"\"\n",
    "        self.snn_timestep = snn_timestep\n",
    "        \n",
    "        #Create the hidden layer\n",
    "        self.hidden_layer = LIFNeurons(hidden_dimension, vdecay, vth)\n",
    "        \n",
    "        #Create the output layer\n",
    "        self.output_layer = LIFNeurons(output_dimension, vdecay, vth)\n",
    "        \n",
    "        #Create the connection between input and hidden layer\n",
    "        self.input_2_hidden = Connections(input_2_hidden_weight, input_dimension, hidden_dimension)\n",
    "        \n",
    "        #Create the connection between hidden and output layer\n",
    "        self.hidden_2_output = Connections(hidden_2_output_weight, hidden_dimension, output_dimension)\n",
    "        \n",
    "    \n",
    "    def __call__(self, spike_encoding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spike_encoding (ndarray): spike encoding of input\n",
    "        Return:\n",
    "            output: decoded output from the network\n",
    "        \"\"\"\n",
    "        \n",
    "        #Initialize an array to store the decoded network output for all neurons in the output layer\n",
    "        spike_output = np.zeros(self.output_layer.dimension)\n",
    "        \n",
    "        #Loop through the simulation timesteps and process the input at each timestep tt\n",
    "        for tt in range(self.snn_timestep):\n",
    "            #Propagate the input through the input to hidden layer and compute current for hidden layer\n",
    "            hidden_layer_current = self.input_2_hidden(spike_encoding[tt])\n",
    "            # print(\"hidden_layer_current at tt:{}\\n{}\".format(tt, hidden_layer_current))\n",
    "           \n",
    "            #Compute hidden layer spikes \n",
    "            hidden_layer_spikes = self.hidden_layer(hidden_layer_current)\n",
    "            # print(\"hidden_layer_spikes at tt:{}\\n{}\".format(tt, hidden_layer_spikes))\n",
    "\n",
    "            #Propagate hidden layer inputs to output layer and compute current for output layer\n",
    "            output_layer_current = self.hidden_2_output(hidden_layer_spikes)\n",
    "            # print(\"output_layer_current at tt:{}\\n{}\".format(tt, output_layer_current))\n",
    "            \n",
    "            #compute output layer spikes\n",
    "            output_layer_spikes = self.output_layer(output_layer_current)\n",
    "            # print(\"output_layer_spikes at tt:{}\\n{}\".format(tt, output_layer_spikes))\n",
    "            \n",
    "            #Decode spike outputs by summing them up\n",
    "            spike_output = spike_output + output_layer_spikes\n",
    "            # print(\"spike_output at tt:{}\\n{}\".format(tt, spike_output))\n",
    "            \n",
    "        return spike_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the correctness of your class implementation, define the arguments to initialize the SNN. Then initialize the SNN and pass through it random inputs and compute network outputs. You can pick arbitrary values for class arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spike Input:\n",
      " [[0 1 0 0 1 0 1 1 1]\n",
      " [1 1 1 1 0 0 1 1 1]\n",
      " [1 1 1 1 1 0 0 1 1]]\n",
      "Out: [3. 3.]\n"
     ]
    }
   ],
   "source": [
    "#Define the dimensions of the input layer in a variable\n",
    "input_dimension = 9\n",
    "\n",
    "#Define the dimensions of the hidden layer in a variable\n",
    "hidden_dimension = 4\n",
    "\n",
    "#Define the dimensions of the output layer in a variable\n",
    "output_dimension = 2\n",
    "\n",
    "#Define vdecay in a variable\n",
    "vdecay = 0.5\n",
    "\n",
    "#Define vth in a variable\n",
    "vth = 0.5\n",
    "\n",
    "#Define snn_timesteps in a variable\n",
    "snn_timestep = 3\n",
    "\n",
    "#Create random input to hidden layer weights. Numpy random rand function might be useful here\n",
    "input_2_hidden_weight = np.random.rand(hidden_dimension, input_dimension)\n",
    "\n",
    "#Create random hidden to output layer weights. Numpy random rand function might be useful here\n",
    "hidden_2_output_weight = np.random.rand(output_dimension, hidden_dimension)\n",
    "\n",
    "#Create random spike inputs to the network. Numpy random choice function might be useful here\n",
    "spike_encoding = np.random.choice([0,1], (snn_timestep, input_dimension), True, [1-p, p]) # considering 3 timesteps - dimensions 3 x input dimension\n",
    "\n",
    "#Print the inputs\n",
    "print(\"Spike Input:\\n {}\".format(spike_encoding))\n",
    "\n",
    "#Create an SNN object using the class definition and variables defined above\n",
    "snn = SNN(input_2_hidden_weight, hidden_2_output_weight, input_dimension, hidden_dimension, output_dimension, vdecay, vth, snn_timestep)\n",
    "\n",
    "#Pass the random spike inputs through the SNN and print the output of the SNN\n",
    "out = snn(spike_encoding)\n",
    "print(\"Out: {}\".format(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: SNN for Classification of Digits\n",
    "So far we have learnt how to construct SNNs for random inputs. In this exercise, you will use your implementation of SNNs to classify real-world data, taking the dataset of handwritten digits as an example. The dataset is provided as numpy arrays in the folder \"data\". Each sample in the MNIST dataset is a 28x28 image of a digit and a label (between 0 and 9) of that image. We will be dealing with batches, which means that we will read a fixed number of samples from the dataset (also called the batch size).\n",
    "\n",
    "## 4a. \n",
    "First, we need to write two helper functions- to read the data from the saved data files, and to convert an image into spikes. The function to read the data is already written for you. You need to complete the function for encoding the data into spikes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_numpy_mnist_data(save_root, num_sample):\n",
    "    \"\"\"\n",
    "    Read saved numpy MNIST data\n",
    "    Args:\n",
    "        save_root (str): path to the folder where the MNIST data is saved\n",
    "        num_sample (int): number of samples to read\n",
    "    Returns:\n",
    "        image_list: list of MNIST image\n",
    "        label_list: list of corresponding labels\n",
    "    \n",
    "    This function is complete. You do not need to do anything here.\n",
    "    \"\"\"\n",
    "    image_list = np.zeros((num_sample, 28, 28))\n",
    "    label_list = []\n",
    "    for ii in range(num_sample):\n",
    "        image_label = pickle.load(open(save_root + '/' + str(ii) + '.p', 'rb'))\n",
    "        image_list[ii] = image_label[0]\n",
    "        label_list.append(image_label[1])\n",
    "\n",
    "    return image_list, label_list\n",
    "\n",
    "def img_2_event_img(image, snn_timestep=20):\n",
    "    \"\"\"\n",
    "    Transform image to spikes, also called an event image\n",
    "    Args:\n",
    "        image (ndarray): image of shape batch_size x 28 x 28\n",
    "        snn_timestep (int): spike timestep\n",
    "    Returns:\n",
    "        event_image: event image- spike encoding of the image\n",
    "        \n",
    "    Complete the expression for converting the image to spikes (event image)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Reshape the image. Do not touch this code\n",
    "    batch_size = image.shape[0]\n",
    "    image_size = image.shape[2]\n",
    "    image = image.reshape(batch_size, image_size, image_size, 1)\n",
    "    \n",
    "    #Generate a random image of the shape batch_size x image_size x image_size x snn_timestep. Numpy random rand function will be useful here. \n",
    "    ## rand_img = np.random.rand(snn_timestep, batch_size, image_size*image_size)\n",
    "    rand_img = np.random.rand(batch_size, snn_timestep, image_size*image_size)\n",
    "\n",
    "    #Generate the event image\n",
    "    img = image.reshape(batch_size, image_size*image_size)\n",
    "    ## img = np.repeat(img[np.newaxis, :, :], snn_timestep, axis=0)\n",
    "    img = np.repeat(img[:, np.newaxis, :], snn_timestep, axis=1)\n",
    "    event_image = (img > rand_img).astype(int)\n",
    "\n",
    "    return event_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the correctness of your class implementation, load a sample digit from the saved file and convert it into an event image. Then print the shape of the event image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data Shape: [batch_size x img_width x img_height] (1000, 28, 28)\n",
      "Event Image Shape: [batch_size x timestep x flattened_image] (1000, 20, 784)\n"
     ]
    }
   ],
   "source": [
    "#Load 1000 samples from the MNIST dataset using the read function defined above\n",
    "num_sample = 1000\n",
    "save_root = \"data/mnist_test\"\n",
    "X, y = read_numpy_mnist_data(save_root, num_sample)\n",
    "\n",
    "#Print the shape of the data\n",
    "print(\"Input Data Shape: [batch_size x img_width x img_height] {}\".format(X.shape))\n",
    "\n",
    "#Convert the images to event images\n",
    "event_img = img_2_event_img(X)\n",
    "\n",
    "#Print the shape of the event image\n",
    "print(\"Event Image Shape: [batch_size x timestep x flattened_image] {}\".format(event_img.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. \n",
    "Next, we need another helper function to compute the classification accuracy of the network. The classification accuracy is defined as the percentage of the samples that the network classifies correctly. To compute the classification accuracy, you need to:\n",
    "\n",
    "- Propagate each input through the network and obtain the network output.\n",
    "- Based on the network output, the class of the image is the one for which the output neuron has maximum value. Let's call this predicted class. \n",
    "- Compare the predicted class against the true class. \n",
    "- Compute accuracy as the percentage of correct predictions. \n",
    "\n",
    "Below is the function for computing the test accuracy. The function takes in as arguments the SNN, directory in which the MNIST data is saved, and the number of samples to take from the MNIST dataset. Your task is to use the helper functions created above to load the data, convert into event images, and then compute network prediction and accuracies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_snn_with_mnist(network, data_save_dir, data_sample_num):\n",
    "    \"\"\"\n",
    "    Test SNN with MNIST test data\n",
    "    Args:\n",
    "        network (SNN): defined SNN network\n",
    "        data_save_dir (str): directory for the test data\n",
    "        data_sample_num (int): number of test data examples\n",
    "    \"\"\"\n",
    "    #Read image and labels using the read function\n",
    "    test_image_list, test_label_list = read_numpy_mnist_data(data_save_dir, data_sample_num)\n",
    "    \n",
    "    #Convert the images to event images\n",
    "    event_image = img_2_event_img(test_image_list)\n",
    "    \n",
    "    ## out = network(event_image)\n",
    "    ## predictions = np.argmax(out, axis=1)\n",
    "\n",
    "    #Initialize number of correct predictions to 0\n",
    "    correct_prediction = 0\n",
    "    ## correct_prediction += sum((predictions == test_label_list).astype(int))\n",
    "\n",
    "    #Loop through the test images\n",
    "    for ii in range(data_sample_num):\n",
    "        #Compute network output for each image. You might have to reshape the image using Numpy reshape function so that its appropriate for the SNN\n",
    "        out = network(event_image[ii])\n",
    "        \n",
    "        #Determine the class of the image from the network output. Numpy argmax function might be useful here\n",
    "        predictions = np.argmax(out, axis=0)\n",
    "        \n",
    "        #Compare the predicted class against true class and update correct_prediction counter\n",
    "        if predictions == test_label_list[ii]:\n",
    "            correct_prediction += 1\n",
    "        \n",
    "    #Compute test accuracy\n",
    "    \n",
    "    test_accuracy = correct_prediction/data_sample_num\n",
    "    print(\"{}/{} = {}\".format(correct_prediction,len(test_label_list),test_accuracy))\n",
    "    # for i in range(len(predictions)):\n",
    "    #     print(\"predicted = {} | true label = {}\".format(predictions[i], test_label_list[i]))\n",
    "\n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Tuning Membrane Properties for Correct Classification \n",
    "Great! We have everything that we need to measure the performance of the SNN for classification of MNIST digits. For this, we first need to create the SNN using the class definition we wrote in Q.3. Then we need to call the test function that we wrote in Q.4b. However, note that the SNN needs the connection weights between the layers as inputs. These weights are typically obtained as a result of \"training\" the network for a given task (such as MNIST classification). However, since training the network isn't a part of this assignment, we provide to you already trained weights. \n",
    "\n",
    "## 5a. \n",
    "Your task in this exercise is to initialize an SNN with vdecay=1.0 and vth=0.5. Test the SNN on MNIST dataset and obtain the classification accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241/1000 = 0.241\n",
      "Test Accuracy = 0.241\n"
     ]
    }
   ],
   "source": [
    "#Load the weights. Do not touch this code\n",
    "snn_param_dir = 'save_models/snn_bptt_mnist_train.p'\n",
    "snn_param_dict = pickle.load(open(snn_param_dir, 'rb'))\n",
    "input_2_hidden_weight = snn_param_dict['weight1']\n",
    "hidden_2_output_weight = snn_param_dict['weight2']\n",
    "\n",
    "#Define a variable for vdecay\n",
    "vdecay = 1.0\n",
    "\n",
    "#Define a variable for vth\n",
    "vth = 0.5\n",
    "\n",
    "#Define parameters\n",
    "data_save_dir = \"data/mnist_test\"\n",
    "data_sample_num = 1000\n",
    "input_dimension=784 \n",
    "hidden_dimension=256 \n",
    "output_dimension=10\n",
    "snn_timestep=20\n",
    "\n",
    "#Create the SNN using the class definition in Q3 and the variables defined above\n",
    "network = SNN(input_2_hidden_weight, hidden_2_output_weight, input_dimension, hidden_dimension, output_dimension, vdecay, vth, snn_timestep)\n",
    "\n",
    "#Compute test accuracy for the SNN on 1000 examples from MNIST dataset and print it\n",
    "test_accuracy = test_snn_with_mnist(network, data_save_dir, data_sample_num)\n",
    "print(\"Test Accuracy = {}\".format(test_accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What could be a possible reason for the poor accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 5a. \n",
    "> As we have fixed our vdecay to 1.0, the voltage does not decay and after a point there is continuous spike for every input which is unable to then differentiate between them and hence we have a poor accuracy. We decrease vdecay to 0.5 or 0.6 and then we will be able to get a much better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. \n",
    "Can you tune the membrane properties (vdecay and vth) to obtain higher classification accuracies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "970/1000 = 0.97\n",
      "Tuned Test Accuracy = 0.97\n"
     ]
    }
   ],
   "source": [
    "#Write your implementation of Question 5b. here\n",
    "vdecay = 0.5\n",
    "vth = 0.5\n",
    "tuned_snn = SNN(input_2_hidden_weight, hidden_2_output_weight, input_dimension, hidden_dimension, output_dimension, vdecay, vth)\n",
    "tuned_accuracy = test_snn_with_mnist(tuned_snn, data_save_dir, data_sample_num)\n",
    "print(\"Tuned Test Accuracy = {}\".format(tuned_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5c.\n",
    "Based on your response to Questions 5a. and 5b., can you explain how membrane properties affect network activity for classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 5c.\n",
    "> With varying membrane properties, we get a pattern of varying spikes from different neurons. It is this unique combination of different neuron spikes that help us to classify different digits. These neuronal spikes are governed by membrane properties like vdecay and vthreshold. Setting our vthreshold too high or too low can result in very bad spiking behaviour, similarly setting vdecay too high or too low can have same effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
